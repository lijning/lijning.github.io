# Machine Learning: E-M algorithm

## Maximum likelihood

- Probabilistic objective function
- i.i.d. assumption of data $x_i$ 
- Simple case: model parameters set $\theta$

### Coordinate ascent

![image-20200404102353266](../../img/2020-04-04-ML-EM-algo/image-20200404102353266.png)

In more complicated cases... Although we can solve one given the other, we canâ€™t solve it simultaneously

![image-20200404102534622](../../img/2020-04-04-ML-EM-algo/image-20200404102534622.png)

Similar to K-means.

## Expectation-Maximization algorithm

### Example: $x_i\in R$ with missing data

:question: how is it related to "the third" situation? lec 20 p7

Supposing we imputed the missing $x^m$, we can calculate the parameters easily.

![image-20200404104338978](../../img/2020-04-04-ML-EM-algo/image-20200404104338978.png)

For one observation $x_i$,

![image-20200404104705369](../../img/2020-04-04-ML-EM-algo/image-20200404104705369.png)

The $x^m$ here is treated as some **unknown while unwanted parameters** $\theta_2$.

- EM is a method than can **both learn parameters and <u>impute</u> missing values** at the same time.

### Objective function

![image-20200404111046713](../../img/2020-04-04-ML-EM-algo/image-20200404111046713.png)

We want a iterative method:

- $\ln p(x|\theta_1^{(t)})>\ln p(x|\theta_1^{(t-1)})$ 
- Converge at local maximum

### Algorithm

- E: supposing the $q(\theta_2)$ conditional on $\theta_1^{(t)}$, we have $L(\theta_1)\propto \ln p(x|\theta_1)$.
  - Setting $q(\theta_2)=p(\theta_2|x,\theta_1)$, so the above KL term values 0.
- M: argmax over $\theta_1$ of $L(\theta_1)$, we get $\theta_1^{(t+1)}$.

### Back to Missing values example

- :question: Did we just use the empirical mean to impute the missing data $x_i^m$?

